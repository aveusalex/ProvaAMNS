{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c700ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.cuda import device\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "%matplotlib inline\n",
    "import numpy, scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affcda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "               out_channels=256,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "        self.dropout = nn.Dropout()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(8192, 7)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bcd4ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNNetwork(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear): Linear(in_features=8192, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNNNetwork().to(device)\n",
    "cnn.load_state_dict(torch.load('cnnmelhorprovaamns.pth'))\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da37184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada:  torch.Size([1, 1, 64000])\n",
      "Saída: torch.Size([1, 1, 64000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers, 3 layers\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, 1000, stride=64, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Conv1d(8, 16, 100, stride=4, padding=0),\n",
    "            # nn.BatchNorm1d(16),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Conv1d(16, 32, 50, stride=4, padding=0),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Conv1d(32, 64, 50, stride=2, padding=0),\n",
    "            # nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Flatten layer\n",
    "        # self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        # Linear section\n",
    "        # self.encoder_lin = nn.Sequential(\n",
    "        #     nn.Linear(496, encoded_space_dim),\n",
    "        #     nn.ReLU(True),\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.flatten(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.encoder_lin(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        # self.decoder_lin = nn.Sequential(\n",
    "        #     nn.Linear(encoded_space_dim, 496),\n",
    "        #     nn.ReLU(True)\n",
    "        # )\n",
    "        #\n",
    "        # self.unflatten = nn.Unflatten(dim=1,\n",
    "        #                               unflattened_size=(8, 62))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(64, 32, 50,\n",
    "            #                    stride=2, padding=0, output_padding=0),\n",
    "            # nn.BatchNorm1d(32),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.ConvTranspose1d(32, 16, 50, stride=4,\n",
    "            #                    padding=0, output_padding=0),\n",
    "            # nn.BatchNorm1d(16),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.ConvTranspose1d(16, 8, 100, stride=4,\n",
    "            #                    padding=0, output_padding=0),\n",
    "            # nn.BatchNorm1d(8),\n",
    "            # nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(64, 1, 1024, stride=64,\n",
    "                               padding=0, output_padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.decoder_lin(x)\n",
    "        # x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from time import time\n",
    "    # em gpu\n",
    "    x = torch.randn(1, 1, 4*16000).cuda()\n",
    "    print(\"Entrada: \", x.shape)\n",
    "    encoder = Encoder(64).cuda()\n",
    "    decoder = Decoder(64).cuda()\n",
    "    x_ = encoder(x)\n",
    "    x_ = decoder(x_)\n",
    "    print(\"Saída:\", x_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a674bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (encoder_cnn): Sequential(\n",
       "    (0): Conv1d(1, 64, kernel_size=(1000,), stride=(64,))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('encoderRede_basicona_tanh.pth'))\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86529a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (decoder_conv): Sequential(\n",
       "    (0): ConvTranspose1d(64, 1, kernel_size=(1024,), stride=(64,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.load_state_dict(torch.load('decoderRede_basicona_tanh.pth'))\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6bd408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DadosAE(Dataset):\n",
    "    def __init__(self,\n",
    "               target_sample_rate):\n",
    "        self.raiz = r\"C:\\Users\\heloi\\OneDrive\\Área de Trabalho\\BIA 6 PERIODO\\ANS\"\n",
    "        self.annotations = pd.read_csv(r\"C:\\Users\\heloi\\OneDrive\\Área de Trabalho\\BIA 6 PERIODO\\ANS\\metadata/testeravdess.csv\")\n",
    "        self.annotations_ruido = pd.read_csv(r\"C:\\Users\\heloi\\OneDrive\\Área de Trabalho\\BIA 6 PERIODO\\ANS\\metadata/testeravdessruidoso.csv\")\n",
    "        self.device = \"cuda\"\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = 4 * 16000\n",
    "        self.cache = dict()  # controlamos para ter no máximo 100 amostras no cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_ruido)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index in self.cache:\n",
    "            return self.cache[index]\n",
    "        path_ruido, path_limpo = self._get_audio_sample_path(index)\n",
    "\n",
    "        # audio com ruido (X)\n",
    "        signal, sr = torchaudio.load(self.raiz + \"/\" + path_ruido)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "\n",
    "        # audio limpo (y)\n",
    "        signal_limpo, sr_limpo = torchaudio.load(self.raiz + \"/\" + path_limpo)\n",
    "        signal_limpo = signal_limpo.to(self.device)\n",
    "        signal_limpo = self._resample_if_necessary(signal_limpo, sr_limpo)\n",
    "        signal_limpo = self._mix_down_if_necessary(signal_limpo)\n",
    "        signal_limpo = self._cut_if_necessary(signal_limpo)\n",
    "        signal_limpo = self._right_pad_if_necessary(signal_limpo)\n",
    "\n",
    "        if len(self.cache) < 256:\n",
    "            self.cache[index] = (signal, signal_limpo)\n",
    "        else:\n",
    "            self.cache = dict()\n",
    "            self.cache[index] = (signal, signal_limpo)\n",
    "\n",
    "        return signal, signal_limpo\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            resampler.to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        return self.annotations_ruido.iloc[index, 0], self.annotations.iloc[index, 0]\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ab2c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe para criação do dataloader, com ela conseguimos abrir os arquivos de áudio a serem utilizados\n",
    "from torch.utils.data import Dataset\n",
    "class SerDataset (Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.cache = dict()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if index in self.cache:\n",
    "          return self.cache[index]\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        label_dict = {\"fear\":0, \"disgust\":1, \"happy\":2, \"sad\":3, \"neutral\":4, \"angry\":5, \"surprise\":6}\n",
    "        label2 = label_dict[label]\n",
    "        self.cache[index] = signal, label2\n",
    "        return signal, label2\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            resampler.to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self,index):\n",
    "        return self.annotations.iloc[index,0]\n",
    "    def _get_audio_sample_label(self,index):\n",
    "        return self.annotations.iloc[index,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02e1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataautoencoder = DadosAE(target_sample_rate=16000)\n",
    "batch_size = 16\n",
    "test_loaderautoencoder = DataLoader(test_dataautoencoder, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d6d6d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "#Acessando a gpu\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1a5c03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1144 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "#Captando os dados de teste com ruído\n",
    "ANNOTATIONS_FILE = \"metadata/testeravdessruidoso.csv\"\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = 4*16000\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "test_data = SerDataset(ANNOTATIONS_FILE,mel_spectrogram,SAMPLE_RATE,NUM_SAMPLES,device)\n",
    "\n",
    "print(f\"There are {len(test_data)} samples in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea661a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando o dataloader\n",
    "test_loader= DataLoader(dataset = test_data, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba8cabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sessão de teste com ruído\n",
    "\n",
    "#lista para armazenar os y verdadeiros\n",
    "y_truetest = []\n",
    "#lista para armazenar os y preditos\n",
    "y_predtest = []\n",
    "\n",
    "cnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for inp_test, label_test in test_loader:\n",
    "                \n",
    "      #alocando o input e a label na GPU\n",
    "      inp_test = inp_test.to(device=device)\n",
    "      label_test = label_test.to(device=device)\n",
    "      \n",
    "      #predição do modelo em cima do input\n",
    "      preds3 = cnn(inp_test)\n",
    "      preds3 = nn.functional.softmax(preds3, dim=1)\n",
    "\n",
    "      #preenchendo as listas referente ao y verdadeiro e y predito\n",
    "      for i in range(len(label_test)):\n",
    "        y_truetest.append(int(label_test[i].item()))\n",
    "        y_predtest.append(int(torch.argmax(preds3[i]).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10c9e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista para armazenar os y preditos\n",
    "y_predtestautoencoder = []\n",
    "\n",
    "cnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for input, target in test_loaderautoencoder:\n",
    "    encoded_Data = encoder(input)\n",
    "    decoded_Data = decoder(encoded_Data)\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "    mel_spectrogram.to(device)\n",
    "    signal = mel_spectrogram(decoded_Data)\n",
    "            \n",
    "      \n",
    "    #predição do modelo em cima do input\n",
    "    predsautoencoder = cnn(signal)\n",
    "    predsautoencoder = nn.functional.softmax(predsautoencoder, dim=1)\n",
    "\n",
    "    #preenchendo as listas referente ao y verdadeiro e y predito\n",
    "    for i in range(len(target)):\n",
    "        y_predtestautoencoder.append(int(torch.argmax(predsautoencoder[i]).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df4fa523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heloi\\AppData\\Local\\Temp\\ipykernel_21536\\2831946722.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_truetest = torch.tensor(y_truetest)\n"
     ]
    }
   ],
   "source": [
    "y_truetest = torch.tensor(y_truetest)\n",
    "y_predtestautoencoder = torch.tensor(y_predtestautoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03e5b447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1144])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predtestautoencoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feb87052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1144])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_truetest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d45f3282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0, 121,  26,  24,   5,   0],\n",
       "        [  0,   1, 135,  23,  12,   5,   0],\n",
       "        [  0,   0, 119,  23,  29,   5,   0],\n",
       "        [  0,   0, 136,  15,  19,   6,   0],\n",
       "        [  0,   2,  59,  14,  10,   3,   0],\n",
       "        [  0,   0, 135,  17,  15,   9,   0],\n",
       "        [  1,   1, 121,  23,  20,  10,   0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "##Vendo os resultados/métricas para o teste com ruído\n",
    "#montando a matriz de confusão\n",
    "confmat = ConfusionMatrix(num_classes=7)\n",
    "confmat(y_predtestautoencoder, y_truetest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa06117b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrizconfusao = confmat(y_predtestautoencoder,y_truetest)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=np.array(matrizconfusao))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14557493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear       0.00      0.00      0.00       176\n",
      "     Disgust       0.25      0.01      0.01       176\n",
      "       Happy       0.14      0.68      0.24       176\n",
      "         Sad       0.11      0.09      0.09       176\n",
      "     Neutral       0.08      0.11      0.09        88\n",
      "       Angry       0.21      0.05      0.08       176\n",
      "    Surprise       1.00      0.00      0.00       176\n",
      "\n",
      "    accuracy                           0.13      1144\n",
      "   macro avg       0.26      0.13      0.07      1144\n",
      "weighted avg       0.27      0.13      0.07      1144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#{\"fear\":0, \"disgust\":1, \"happy\":2, \"sad\":3, \"neutral\":4, \"angry\":5, \"surprise\":6}\n",
    "target_names = ['Fear', 'Disgust', 'Happy','Sad','Neutral','Angry','Surprise']\n",
    "report = classification_report(np.array(y_truetest),np.array(y_predtestautoencoder),target_names= target_names,zero_division=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497d58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
